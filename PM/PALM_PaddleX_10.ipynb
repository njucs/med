{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/med/blob/master/PM/PALM_PaddleX_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTlR2fqwJoFL"
      },
      "source": [
        "**本方案使用TNT模型进行训练与预测，在有限训练次数下，取得了较稳定的成绩：0.99515**\n",
        "\n",
        "**采取的训练图像预处理方案：**\n",
        "\n",
        "- 随机垂直翻转\n",
        "- 随机角度翻转--0~20度\n",
        "- 缩放大小--（520，520）\n",
        "- 归一化--mean:[0.2, 0.3, 0.5], std:[0., 0., 0.]\n",
        "\n",
        "**采取的验证与预测图像预处理方案：**\n",
        "\n",
        "- 缩放大小--（520，520）\n",
        "- 归一化--mean:[0.2, 0.3, 0.5], std:[0., 0., 0.]\n",
        "- 数据集划分比例：0.8\n",
        "\n",
        "**TNT模型比较：**\n",
        "\n",
        "- 微调参数： tnt_s_patch16_224 得分 > tnt_b_patch16_224 得分\n",
        "\n",
        "**后期优化方向：**\n",
        "\n",
        "- 更合适的处理方式\n",
        "- patch大小与数量\n",
        "- 图片输入大小等"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_UpfeIEdr3g"
      },
      "source": [
        "## PaddleX配置"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXF3enlRdtmw"
      },
      "source": [
        "### paddlex安装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Obdbrdr1lePI"
      },
      "outputs": [],
      "source": [
        "# install PaddlePaddle-GPU\n",
        "!python -m pip install paddlepaddle-gpu==2.2.2 -i https://mirror.baidu.com/pypi/simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "327IadTzQ6N9"
      },
      "outputs": [],
      "source": [
        "!pip install paddlex==1.3.11 -i https://mirror.baidu.com/pypi/simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2raUegcHg0K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import paddle\n",
        "from paddle import nn\n",
        "from paddle import optimizer\n",
        "from paddle import regularizer\n",
        "from paddle import metric\n",
        "from paddle.nn import loss\n",
        "from paddle.nn import Layer\n",
        "\n",
        "from paddle.io import Dataset, DataLoader\n",
        "from paddle.vision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcXcIYl4eInK"
      },
      "source": [
        "## 准备数据集"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFjwp8QieKg5"
      },
      "source": [
        "### 准备数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uNgUT5Pw5VV",
        "outputId": "34780b55-8330-41cc-aa6e-dc313e2fe925"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/Ophthalmology/PathologicMyopia\n"
          ]
        }
      ],
      "source": [
        "# 授权 Colab 访问 Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/MyDrive/'Colab Notebooks'/Ophthalmology/PathologicMyopia/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skjCHL0gyD4v"
      },
      "outputs": [],
      "source": [
        "!ls dataset/Train | wc -w\n",
        "!ls dataset/Train/fundus_image/ | wc -w\n",
        "!ls dataset/PALM-Testing400-Images/ | wc -w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "o488t5FqHml8",
        "outputId": "4d463832-05ec-442d-89f4-3331dc06a893"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d354de27-e51e-4b56-a3f9-9864a2cfc7fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imgName</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>V0001.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>V0002.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>V0003.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>V0004.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>V0005.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d354de27-e51e-4b56-a3f9-9864a2cfc7fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d354de27-e51e-4b56-a3f9-9864a2cfc7fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d354de27-e51e-4b56-a3f9-9864a2cfc7fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     imgName  Label\n",
              "0  V0001.jpg      0\n",
              "1  V0002.jpg      1\n",
              "2  V0003.jpg      1\n",
              "3  V0004.jpg      0\n",
              "4  V0005.jpg      0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Image_path = 'dataset/Train/fundus_image'\n",
        "Train_data = pd.read_excel('dataset/Train/Classification.xlsx')\n",
        "Train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "f-WvQ2VMHpbc",
        "outputId": "d82b1595-90ce-41b3-b3a4-d890c6ab602c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2616e729-5004-4d4b-907a-9db705d04261\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imgName</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dataset/Train/fundus_image/P0182.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dataset/Train/fundus_image/V0005.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dataset/Train/fundus_image/N0129.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dataset/Train/fundus_image/V0140.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dataset/Train/fundus_image/V0037.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2616e729-5004-4d4b-907a-9db705d04261')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2616e729-5004-4d4b-907a-9db705d04261 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2616e729-5004-4d4b-907a-9db705d04261');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                imgName  Label\n",
              "0  dataset/Train/fundus_image/P0182.jpg      1\n",
              "1  dataset/Train/fundus_image/V0005.jpg      0\n",
              "2  dataset/Train/fundus_image/N0129.jpg      0\n",
              "3  dataset/Train/fundus_image/V0140.jpg      1\n",
              "4  dataset/Train/fundus_image/V0037.jpg      0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for i in range(len(Train_data)):\n",
        "    Train_data.iloc[i, 0] = os.path.join(Image_path, Train_data.iloc[i, 0])\n",
        "Train_data = Train_data.sample(frac=1.0).reset_index(drop=True)\n",
        "Train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "9byzefIAHp7e",
        "outputId": "39130f6c-fc4c-453f-a046-a356de9167ef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-aeef9a7d-4011-40ec-83a6-131d1bc11e83\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dataset/PALM-Testing400-Images/T0001.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dataset/PALM-Testing400-Images/T0002.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dataset/PALM-Testing400-Images/T0003.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dataset/PALM-Testing400-Images/T0004.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dataset/PALM-Testing400-Images/T0005.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aeef9a7d-4011-40ec-83a6-131d1bc11e83')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aeef9a7d-4011-40ec-83a6-131d1bc11e83 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aeef9a7d-4011-40ec-83a6-131d1bc11e83');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                          0  1\n",
              "0  dataset/PALM-Testing400-Images/T0001.jpg  0\n",
              "1  dataset/PALM-Testing400-Images/T0002.jpg  0\n",
              "2  dataset/PALM-Testing400-Images/T0003.jpg  0\n",
              "3  dataset/PALM-Testing400-Images/T0004.jpg  0\n",
              "4  dataset/PALM-Testing400-Images/T0005.jpg  0"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Test_data = []\n",
        "Test_path = 'dataset/PALM-Testing400-Images'\n",
        "for _, _, files in os.walk(Test_path):\n",
        "    for i in files:\n",
        "        Test_data.append([i, 0])\n",
        "Test_data = np.asarray(Test_data)\n",
        "Test_data = pd.DataFrame(Test_data)\n",
        "Test_data = Test_data.sort_values(by=0, ascending=True).reset_index(drop=True)\n",
        "for i in range(len(Test_data)):\n",
        "    Test_data.iloc[i, 0] = os.path.join(Test_path, Test_data.iloc[i, 0])\n",
        "Test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7HlY3rgHvuj"
      },
      "outputs": [],
      "source": [
        "class Train_Dataset(Dataset):\n",
        "    '''加载训练集\n",
        "        把数据加载函数拼进来\n",
        "    '''\n",
        "    def __init__(self, df, trans=None):\n",
        "        super(Train_Dataset, self).__init__()\n",
        "\n",
        "        self.df = df\n",
        "        \n",
        "        if trans is None:\n",
        "            self.trans = transforms.Compose([\n",
        "                transforms.RandomVerticalFlip(),\n",
        "                transforms.RandomRotation(20),\n",
        "                transforms.Resize(size=(520, 520)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.2, 0.3, 0.5])\n",
        "            ])\n",
        "        else:\n",
        "            self.trans = trans\n",
        "\n",
        "        self.lens = len(df)\n",
        "\n",
        "    def __getitem__(self, indexs):\n",
        "        im_data, im_label = self._load_img_and_label(self.df, indexs)\n",
        "        im_data = self.trans(im_data)\n",
        "        return im_data, paddle.to_tensor(im_label)\n",
        "\n",
        "    def _load_img_and_label(self, df, index):\n",
        "        '''加载DF中的路径为图片和标签\n",
        "            df: 输入DF\n",
        "            index: 第几条数据\n",
        "            mode: 加载训练集数据模式还是测试集模式--区别在于是否转换数据域\n",
        "        '''\n",
        "        assert index < self.lens, \\\n",
        "            'please check the index, which has more than the dataset length!'\n",
        "\n",
        "        im_data = cv.imread(df.iloc[index, 0], cv.COLOR_BGR2RGB)  # 转为RGB数据\n",
        "        im_label = int(df.iloc[index, 1])  # 标签\n",
        "        return np.asarray(im_data).astype('float32'), im_label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.lens\n",
        "\n",
        "class Test_Dataset(Dataset):\n",
        "    '''加载测试集\n",
        "        把数据加载函数拼进来\n",
        "    '''\n",
        "    def __init__(self, df, trans=None):\n",
        "        super(Test_Dataset, self).__init__()\n",
        "\n",
        "        self.df = df\n",
        "        \n",
        "        if trans is None:\n",
        "            self.trans = transforms.Compose([\n",
        "                transforms.Resize(size=(520, 520)),  # 保证迁移前后输入特征大小一致\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.2, 0.3, 0.5])\n",
        "            ])\n",
        "        else:\n",
        "            self.trans = trans\n",
        "\n",
        "        self.lens = len(df)\n",
        "\n",
        "    def __getitem__(self, indexs):\n",
        "        im_data, im_label = self._load_img_and_label(self.df, indexs)\n",
        "        im_data = self.trans(im_data)\n",
        "        return im_data, paddle.to_tensor(im_label)\n",
        "\n",
        "    def _load_img_and_label(self, df, index):\n",
        "        '''加载DF中的路径为图片和标签\n",
        "            df: 输入DF\n",
        "            index: 第几条数据\n",
        "            mode: 加载训练集数据模式还是测试集模式--区别在于是否转换数据域\n",
        "        '''\n",
        "        assert index < self.lens, \\\n",
        "            'please check the index, which has more than the dataset length!'\n",
        "\n",
        "        im_data = cv.imread(df.iloc[index, 0], cv.COLOR_BGR2RGB)  # 转为RGB数据\n",
        "        im_label = int(df.iloc[index, 1])  # 标签\n",
        "        return np.asarray(im_data).astype('float32'), im_label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scPbUAv_elRe"
      },
      "source": [
        "## 训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33FldCMwIF3t"
      },
      "outputs": [],
      "source": [
        "# 训练参数-=dict\n",
        "Train_Paramdict = {\n",
        "    'data_length':len(Train_data),  # 数据长度\n",
        "    'train_frac':0.8,              # 训练集比例\n",
        "    'num_class':2,                  # 类别\n",
        "    'epoches':30,                   # 训练轮次\n",
        "    'batchsize':8,                 # 批量大小\n",
        "    'lr':0.0001,                      # 学习率\n",
        "    'l2':0.0005                    # L2正则化参数\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wrzhd0t6IIUu"
      },
      "outputs": [],
      "source": [
        "# 数据集划分\n",
        "Fit_data  = Train_data.iloc[:int(Train_Paramdict['data_length']*Train_Paramdict['train_frac'])]\n",
        "Eval_data = Train_data.iloc[int(Train_Paramdict['data_length']*Train_Paramdict['train_frac']):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8cWKOtzIKip"
      },
      "outputs": [],
      "source": [
        "# 数据加载\n",
        "Fit_dataset = Train_Dataset(Fit_data)\n",
        "Eval_dataset = Test_Dataset(Eval_data)\n",
        "All_dataset = Train_Dataset(Train_data)\n",
        "\n",
        "Fit_dataloader = DataLoader(Fit_dataset, batch_size=Train_Paramdict['batchsize'], shuffle=True)\n",
        "Eval_dataloader = DataLoader(Eval_dataset, batch_size=Train_Paramdict['batchsize'])\n",
        "All_dataloader = DataLoader(All_dataset, batch_size=Train_Paramdict['batchsize'], shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oucAFJgNrpF"
      },
      "outputs": [],
      "source": [
        "import paddle\n",
        "from paddle import nn\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def _cfg(url='', **kwargs):\n",
        "    return {\n",
        "        'url': url,\n",
        "        'num_classes': 2, 'input_size': (3, 600, 600), 'pool_size': None,\n",
        "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
        "        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n",
        "        'first_conv': 'pixel_embed.proj', 'classifier': 'head',\n",
        "        **kwargs\n",
        "    }\n",
        "\n",
        "\n",
        "default_cfgs = {\n",
        "    'tnt_s_patch16_224': _cfg(\n",
        "        url='',\n",
        "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
        "    ),\n",
        "    'tnt_b_patch16_224': _cfg(\n",
        "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "class Identity(nn.Layer):\n",
        "    r\"\"\"A placeholder identity operator that is argument-insensitive.\n",
        "    Args:\n",
        "        args: any argument (unused)\n",
        "        kwargs: any keyword argument (unused)\n",
        "    Examples::\n",
        "        >>> m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)\n",
        "        >>> input = torch.randn(128, 20)\n",
        "        >>> output = m(input)\n",
        "        >>> print(output.size())\n",
        "        torch.Size([128, 20])\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + paddle.rand(shape=shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor()  # binarize\n",
        "    output = x.divide(keep_prob) * random_tensor\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Layer):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class Attention(nn.Layer):\n",
        "    '''\n",
        "        注意力部分\n",
        "    '''\n",
        "\n",
        "    def __init__(self, dim, hidden_dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = hidden_dim // num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qk = nn.Linear(dim, hidden_dim * 2, bias_attr=qkv_bias)\n",
        "        self.v = nn.Linear(dim, dim, bias_attr=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)  # no inplace\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        B, N, C = x.shape\n",
        "        qk = self.qk(x).reshape((B, N, 2, self.num_heads, self.head_dim)).transpose((2, 0, 3, 1, 4))\n",
        "        q, k = qk[0], qk[1]\n",
        "        v = self.v(x).reshape((B, N, self.num_heads, -1)).transpose((0, 2, 1, 3))\n",
        "\n",
        "        attn = paddle.matmul(q, k.transpose((0, 1, 3, 2))) * self.scale\n",
        "        attn = paddle.nn.functional.softmax(attn, axis=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = paddle.matmul(attn, v).transpose((0, 2, 1, 3)).reshape((B, N, -1))\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Mlp(nn.Layer):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super(Mlp, self).__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Layer):\n",
        "    \"\"\" TNT Block\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, in_dim, num_pixel, num_heads=12, in_num_head=4, mlp_ratio=4.,\n",
        "            qkv_bias=False, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super(Block, self).__init__()\n",
        "        # Inner transformer\n",
        "        self.norm_in = norm_layer(in_dim)\n",
        "        self.attn_in = Attention(\n",
        "            in_dim, in_dim, num_heads=in_num_head, qkv_bias=qkv_bias,\n",
        "            attn_drop=attn_drop, proj_drop=drop)\n",
        "        \n",
        "        self.norm_mlp_in = norm_layer(in_dim)\n",
        "        self.mlp_in = Mlp(in_features=in_dim, hidden_features=int(in_dim * 4),\n",
        "            out_features=in_dim, act_layer=act_layer, drop=drop)\n",
        "        \n",
        "        self.norm1_proj = norm_layer(in_dim)\n",
        "        self.proj = nn.Linear(in_dim * num_pixel, dim, bias_attr=True)\n",
        "        # Outer transformer\n",
        "        self.norm_out = norm_layer(dim)\n",
        "        self.attn_out = Attention(\n",
        "            dim, dim, num_heads=num_heads, qkv_bias=qkv_bias,\n",
        "            attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else Identity()\n",
        "        \n",
        "        self.norm_mlp = norm_layer(dim)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio),\n",
        "            out_features=dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, pixel_embed, patch_embed):\n",
        "        # inner\n",
        "        pixel_embed = pixel_embed + self.drop_path(self.attn_in(self.norm_in(pixel_embed)))\n",
        "        pixel_embed = pixel_embed + self.drop_path(self.mlp_in(self.norm_mlp_in(pixel_embed)))\n",
        "        # outer\n",
        "        B, N, C = patch_embed.shape\n",
        "        patch_embed[:, 1:] = patch_embed[:, 1:] + self.proj(self.norm1_proj(pixel_embed).reshape((B, N - 1, -1)))\n",
        "        patch_embed = patch_embed + self.drop_path(self.attn_out(self.norm_out(patch_embed)))\n",
        "        patch_embed = patch_embed + self.drop_path(self.mlp(self.norm_mlp(patch_embed)))\n",
        "        return pixel_embed, patch_embed\n",
        "\n",
        "\n",
        "class PixelEmbed(nn.Layer):\n",
        "    \"\"\" Image to Pixel Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, in_dim=48, stride=4):\n",
        "        super(PixelEmbed, self).__init__()\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.img_size = img_size\n",
        "        self.num_patches = num_patches\n",
        "        self.in_dim = in_dim\n",
        "        new_patch_size = math.ceil(patch_size / stride)\n",
        "        self.new_patch_size = new_patch_size\n",
        "\n",
        "        self.proj = nn.Conv2D(in_chans, self.in_dim, kernel_size=7, padding=3, stride=stride)\n",
        "\n",
        "    def forward(self, x, pixel_pos):\n",
        "        B, C, H, W = x.shape\n",
        "        assert H == self.img_size and W == self.img_size, \\\n",
        "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size}*{self.img_size}).\"\n",
        "        x = self.proj(x)\n",
        "        x = nn.functional.unfold(x=x, kernel_sizes=self.new_patch_size, strides=self.new_patch_size)\n",
        "\n",
        "        x = x.transpose((0, 2, 1)).reshape((B * self.num_patches, self.in_dim, self.new_patch_size, self.new_patch_size))\n",
        "        x = x + pixel_pos\n",
        "\n",
        "        x = x.reshape((B * self.num_patches, self.in_dim, -1)).transpose((0, 2, 1))\n",
        "        return x\n",
        "\n",
        "\n",
        "class TNT(nn.Layer):\n",
        "    \"\"\" Transformer in Transformer - https://arxiv.org/abs/2103.00112\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, in_dim=48, depth=12,\n",
        "                 num_heads=12, in_num_head=4, mlp_ratio=4., qkv_bias=False, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., norm_layer=nn.LayerNorm, first_stride=4):\n",
        "        super(TNT, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "\n",
        "        self.pixel_embed = PixelEmbed(\n",
        "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, in_dim=in_dim, stride=first_stride)\n",
        "        num_patches = self.pixel_embed.num_patches\n",
        "        self.num_patches = num_patches\n",
        "        new_patch_size = self.pixel_embed.new_patch_size\n",
        "        num_pixel = new_patch_size ** 2\n",
        "        \n",
        "        self.norm1_proj = norm_layer(num_pixel * in_dim)\n",
        "        self.proj = nn.Linear(num_pixel * in_dim, embed_dim)\n",
        "        self.norm2_proj = norm_layer(embed_dim)\n",
        "            \n",
        "        # 创建参数\n",
        "        self.cls_token = paddle.create_parameter((1, 1, embed_dim), 'float32', attr=nn.initializer.Assign(paddle.zeros((1, 1, embed_dim))))\n",
        "        self.patch_pos = paddle.create_parameter((1, num_patches + 1, embed_dim), 'float32', attr=nn.initializer.Assign(paddle.zeros((1, num_patches + 1, embed_dim))))\n",
        "        self.pixel_pos = paddle.create_parameter((1, in_dim, new_patch_size, new_patch_size), 'float32', attr=nn.initializer.Assign(paddle.zeros((1, in_dim, new_patch_size, new_patch_size))))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        dpr = [x for x in paddle.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        blocks = []\n",
        "        for i in range(depth):\n",
        "            blocks.append(Block(\n",
        "                dim=embed_dim, in_dim=in_dim, num_pixel=num_pixel, num_heads=num_heads, in_num_head=in_num_head,\n",
        "                mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[i], norm_layer=norm_layer))\n",
        "        self.blocks = nn.LayerList(blocks)\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        with paddle.no_grad():\n",
        "            self.cls_token = paddle.create_parameter(self.cls_token.shape, 'float32', attr=nn.initializer.Assign(paddle.normal(self.cls_token, std=.02)))\n",
        "            self.patch_pos = paddle.create_parameter(self.patch_pos.shape, 'float32', attr=nn.initializer.Assign(paddle.normal(self.patch_pos, std=.02)))\n",
        "            self.pixel_pos = paddle.create_parameter(self.pixel_pos.shape, 'float32', attr=nn.initializer.Assign(paddle.normal(self.pixel_pos, std=.02)))\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            with paddle.no_grad():\n",
        "                m.weight = paddle.create_parameter(m.weight.shape, 'float32', attr=nn.initializer.Assign(paddle.normal(m.weight, std=.02)))\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                m.bias = paddle.create_parameter(m.bias.shape, 'float32', attr=nn.initializer.Constant(value=0.))\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            m.bias = paddle.create_parameter(m.bias.shape, 'float32', attr=nn.initializer.Constant(value=0.))\n",
        "            m.weight = paddle.create_parameter(m.weight.shape, 'float32', attr=nn.initializer.Constant(value=1.))\n",
        "\n",
        "    def no_weight_decay(self):\n",
        "        return {'patch_pos', 'pixel_pos', 'cls_token'}\n",
        "\n",
        "    def get_classifier(self):\n",
        "        return self.head\n",
        "\n",
        "    def reset_classifier(self, num_classes, global_pool=''):\n",
        "        self.num_classes = num_classes\n",
        "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "        pixel_embed = self.pixel_embed(x, self.pixel_pos)\n",
        "        \n",
        "        patch_embed = self.norm2_proj(self.proj(self.norm1_proj(pixel_embed.reshape((B, self.num_patches, -1)))))\n",
        "        patch_embed = paddle.concat((self.cls_token.expand([B, self.cls_token.shape[1],self.cls_token.shape[2]]), patch_embed), axis=1)  # expand\n",
        "        patch_embed = patch_embed + self.patch_pos\n",
        "        patch_embed = self.pos_drop(patch_embed)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            pixel_embed, patch_embed = blk(pixel_embed, patch_embed)\n",
        "\n",
        "        patch_embed = self.norm(patch_embed)\n",
        "        return patch_embed[:, 0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def tnt_s_patch16_224(pretrained=False, **kwargs):\n",
        "    model = TNT(patch_size=16, embed_dim=384, in_dim=24, depth=12, num_heads=6, in_num_head=4,\n",
        "        qkv_bias=False, **kwargs)\n",
        "    model.default_cfg = default_cfgs['tnt_s_patch16_224']\n",
        "    if pretrained:\n",
        "        load_pretrained(\n",
        "            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n",
        "    return model\n",
        "\n",
        "\n",
        "def tnt_b_patch16_224(pretrained=False, **kwargs):\n",
        "    model = TNT(patch_size=16, embed_dim=640, in_dim=40, depth=12, num_heads=10, in_num_head=4,\n",
        "        qkv_bias=False, **kwargs)\n",
        "    model.default_cfg = default_cfgs['tnt_b_patch16_224']\n",
        "    if pretrained:\n",
        "        load_pretrained(\n",
        "            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kggqrRytemhy"
      },
      "outputs": [],
      "source": [
        "# 创建模型\n",
        "#import tnt_s_patch16_224, tnt_b_patch16_224\n",
        "model = tnt_s_patch16_224(img_size=520, num_classes=2)\n",
        "model = paddle.Model(model)\n",
        "\n",
        "lr = optimizer.lr.LinearWarmup(\n",
        "    learning_rate=Train_Paramdict['lr'],\n",
        "    warmup_steps = 2000,\n",
        "    start_lr = 0, \n",
        "    end_lr = Train_Paramdict['lr']\n",
        ")\n",
        "\n",
        "O = optimizer.Adam(lr, parameters=model.parameters(), weight_decay=regularizer.L2Decay(Train_Paramdict['l2']))\n",
        "L = loss.CrossEntropyLoss()\n",
        "M = metric.Accuracy()\n",
        "\n",
        "model.prepare(O, L, M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxf3l3cdIQ2O",
        "outputId": "7aa17c5a-466a-4a40-f081-4f122f8b15be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/paddle/tensor/creation.py:130: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if data.dtype == np.object:\n",
            "/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  return (isinstance(seq, collections.Sequence) and\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 10/80 - loss: 1.4929 - acc: 0.5375 - 6s/step\n",
            "step 20/80 - loss: 0.4444 - acc: 0.5250 - 6s/step\n",
            "step 30/80 - loss: 0.5149 - acc: 0.6458 - 7s/step\n",
            "step 40/80 - loss: 0.4985 - acc: 0.6562 - 7s/step\n",
            "step 50/80 - loss: 0.3731 - acc: 0.7000 - 7s/step\n",
            "step 60/80 - loss: 0.5611 - acc: 0.7125 - 7s/step\n",
            "step 70/80 - loss: 0.8010 - acc: 0.7196 - 7s/step\n",
            "step 80/80 - loss: 0.4757 - acc: 0.7266 - 7s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.3501 - acc: 0.8500 - 5s/step\n",
            "step 20/20 - loss: 0.1822 - acc: 0.8750 - 5s/step\n",
            "Eval samples: 160\n",
            "Epoch 2/100\n",
            "step 10/80 - loss: 0.2001 - acc: 0.8125 - 3s/step\n",
            "step 20/80 - loss: 0.1912 - acc: 0.8313 - 3s/step\n",
            "step 30/80 - loss: 0.4696 - acc: 0.8250 - 3s/step\n",
            "step 40/80 - loss: 0.2640 - acc: 0.8281 - 3s/step\n",
            "step 50/80 - loss: 0.3740 - acc: 0.8350 - 3s/step\n",
            "step 60/80 - loss: 0.2008 - acc: 0.8417 - 3s/step\n",
            "step 70/80 - loss: 0.6242 - acc: 0.8357 - 3s/step\n",
            "step 80/80 - loss: 0.1768 - acc: 0.8422 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.3440 - acc: 0.8625 - 1s/step\n",
            "step 20/20 - loss: 0.0852 - acc: 0.8812 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 3/100\n",
            "step 10/80 - loss: 0.2352 - acc: 0.8375 - 3s/step\n",
            "step 20/80 - loss: 0.3684 - acc: 0.8187 - 3s/step\n",
            "step 30/80 - loss: 0.2358 - acc: 0.8417 - 3s/step\n",
            "step 40/80 - loss: 0.2473 - acc: 0.8438 - 3s/step\n",
            "step 50/80 - loss: 0.5083 - acc: 0.8375 - 3s/step\n",
            "step 60/80 - loss: 0.4967 - acc: 0.8313 - 3s/step\n",
            "step 70/80 - loss: 0.2272 - acc: 0.8304 - 3s/step\n",
            "step 80/80 - loss: 0.2652 - acc: 0.8359 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.3373 - acc: 0.8500 - 1s/step\n",
            "step 20/20 - loss: 0.1302 - acc: 0.8750 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 4/100\n",
            "step 10/80 - loss: 0.2968 - acc: 0.9125 - 3s/step\n",
            "step 20/80 - loss: 0.7839 - acc: 0.8625 - 3s/step\n",
            "step 30/80 - loss: 0.4201 - acc: 0.8417 - 3s/step\n",
            "step 40/80 - loss: 0.2466 - acc: 0.8250 - 3s/step\n",
            "step 50/80 - loss: 0.4289 - acc: 0.8250 - 3s/step\n",
            "step 60/80 - loss: 0.3419 - acc: 0.8333 - 3s/step\n",
            "step 70/80 - loss: 0.2016 - acc: 0.8393 - 3s/step\n",
            "step 80/80 - loss: 0.3854 - acc: 0.8422 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.5427 - acc: 0.7625 - 1s/step\n",
            "step 20/20 - loss: 0.3011 - acc: 0.7812 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 5/100\n",
            "step 10/80 - loss: 0.1119 - acc: 0.8250 - 3s/step\n",
            "step 20/80 - loss: 0.2176 - acc: 0.8500 - 3s/step\n",
            "step 30/80 - loss: 0.5037 - acc: 0.8458 - 3s/step\n",
            "step 40/80 - loss: 0.4877 - acc: 0.8438 - 3s/step\n",
            "step 50/80 - loss: 0.1223 - acc: 0.8300 - 3s/step\n",
            "step 60/80 - loss: 0.4517 - acc: 0.8354 - 3s/step\n",
            "step 70/80 - loss: 0.1891 - acc: 0.8482 - 3s/step\n",
            "step 80/80 - loss: 0.2238 - acc: 0.8516 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.4212 - acc: 0.8750 - 1s/step\n",
            "step 20/20 - loss: 0.0158 - acc: 0.8875 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 6/100\n",
            "step 10/80 - loss: 0.5116 - acc: 0.8875 - 3s/step\n",
            "step 20/80 - loss: 0.1187 - acc: 0.8875 - 3s/step\n",
            "step 30/80 - loss: 0.2532 - acc: 0.9125 - 3s/step\n",
            "step 40/80 - loss: 0.3168 - acc: 0.9062 - 3s/step\n",
            "step 50/80 - loss: 0.7332 - acc: 0.9100 - 3s/step\n",
            "step 60/80 - loss: 0.1912 - acc: 0.8958 - 3s/step\n",
            "step 70/80 - loss: 0.2730 - acc: 0.8982 - 3s/step\n",
            "step 80/80 - loss: 0.1742 - acc: 0.8969 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.2752 - acc: 0.9125 - 1s/step\n",
            "step 20/20 - loss: 0.0121 - acc: 0.9125 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 7/100\n",
            "step 10/80 - loss: 0.0694 - acc: 0.9000 - 3s/step\n",
            "step 20/80 - loss: 0.1675 - acc: 0.9313 - 3s/step\n",
            "step 30/80 - loss: 0.0970 - acc: 0.9000 - 3s/step\n",
            "step 40/80 - loss: 0.1754 - acc: 0.8906 - 3s/step\n",
            "step 50/80 - loss: 0.5223 - acc: 0.8900 - 3s/step\n",
            "step 60/80 - loss: 0.4097 - acc: 0.8812 - 3s/step\n",
            "step 70/80 - loss: 0.0976 - acc: 0.8768 - 3s/step\n",
            "step 80/80 - loss: 0.1229 - acc: 0.8734 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.4460 - acc: 0.8625 - 2s/step\n",
            "step 20/20 - loss: 0.0097 - acc: 0.8688 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 8/100\n",
            "step 10/80 - loss: 0.3134 - acc: 0.8875 - 3s/step\n",
            "step 20/80 - loss: 0.6635 - acc: 0.8812 - 3s/step\n",
            "step 30/80 - loss: 0.2646 - acc: 0.8542 - 3s/step\n",
            "step 40/80 - loss: 0.1909 - acc: 0.8781 - 3s/step\n",
            "step 50/80 - loss: 0.1241 - acc: 0.8900 - 3s/step\n",
            "step 60/80 - loss: 0.0930 - acc: 0.8917 - 3s/step\n",
            "step 70/80 - loss: 0.0478 - acc: 0.9018 - 3s/step\n",
            "step 80/80 - loss: 0.0323 - acc: 0.9047 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.1940 - acc: 0.9125 - 1s/step\n",
            "step 20/20 - loss: 0.0116 - acc: 0.9125 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 9/100\n",
            "step 10/80 - loss: 0.1473 - acc: 0.9125 - 3s/step\n",
            "step 20/80 - loss: 0.0699 - acc: 0.9375 - 3s/step\n",
            "step 30/80 - loss: 0.2715 - acc: 0.8917 - 3s/step\n",
            "step 40/80 - loss: 0.1671 - acc: 0.9000 - 3s/step\n",
            "step 50/80 - loss: 0.0598 - acc: 0.8975 - 3s/step\n",
            "step 60/80 - loss: 0.3756 - acc: 0.8938 - 3s/step\n",
            "step 70/80 - loss: 0.6040 - acc: 0.8911 - 3s/step\n",
            "step 80/80 - loss: 0.1409 - acc: 0.8938 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.3316 - acc: 0.9000 - 1s/step\n",
            "step 20/20 - loss: 0.0197 - acc: 0.9062 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 10/100\n",
            "step 10/80 - loss: 0.1431 - acc: 0.8750 - 3s/step\n",
            "step 20/80 - loss: 0.5336 - acc: 0.9000 - 3s/step\n",
            "step 30/80 - loss: 0.2388 - acc: 0.8917 - 3s/step\n",
            "step 40/80 - loss: 0.0946 - acc: 0.9125 - 3s/step\n",
            "step 50/80 - loss: 0.6197 - acc: 0.9050 - 3s/step\n",
            "step 60/80 - loss: 0.9207 - acc: 0.8958 - 3s/step\n",
            "step 70/80 - loss: 0.2442 - acc: 0.9000 - 3s/step\n",
            "step 80/80 - loss: 0.9425 - acc: 0.9031 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.4259 - acc: 0.8625 - 1s/step\n",
            "step 20/20 - loss: 0.0066 - acc: 0.8750 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 11/100\n",
            "step 10/80 - loss: 0.2784 - acc: 0.8500 - 3s/step\n",
            "step 20/80 - loss: 0.4604 - acc: 0.8812 - 3s/step\n",
            "step 30/80 - loss: 0.1380 - acc: 0.8833 - 3s/step\n",
            "step 40/80 - loss: 0.4769 - acc: 0.8844 - 3s/step\n",
            "step 50/80 - loss: 0.0293 - acc: 0.8850 - 3s/step\n",
            "step 60/80 - loss: 0.2370 - acc: 0.8875 - 3s/step\n",
            "step 70/80 - loss: 0.1906 - acc: 0.8857 - 3s/step\n",
            "step 80/80 - loss: 0.3967 - acc: 0.8859 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.2749 - acc: 0.9375 - 1s/step\n",
            "step 20/20 - loss: 0.0340 - acc: 0.9375 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 12/100\n",
            "step 10/80 - loss: 0.1513 - acc: 0.9250 - 3s/step\n",
            "step 20/80 - loss: 0.1157 - acc: 0.9125 - 3s/step\n",
            "step 30/80 - loss: 0.0539 - acc: 0.9208 - 3s/step\n",
            "step 40/80 - loss: 0.2017 - acc: 0.9250 - 3s/step\n",
            "step 50/80 - loss: 0.0527 - acc: 0.9150 - 3s/step\n",
            "step 60/80 - loss: 0.1230 - acc: 0.9187 - 3s/step\n",
            "step 70/80 - loss: 0.5059 - acc: 0.9161 - 3s/step\n",
            "step 80/80 - loss: 0.2281 - acc: 0.9047 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.2575 - acc: 0.9125 - 1s/step\n",
            "step 20/20 - loss: 0.0346 - acc: 0.9187 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 13/100\n",
            "step 10/80 - loss: 0.0987 - acc: 0.9250 - 3s/step\n",
            "step 20/80 - loss: 0.5728 - acc: 0.9187 - 3s/step\n",
            "step 30/80 - loss: 0.0969 - acc: 0.9083 - 3s/step\n",
            "step 40/80 - loss: 0.0694 - acc: 0.9062 - 3s/step\n",
            "step 50/80 - loss: 1.2851 - acc: 0.9100 - 3s/step\n",
            "step 60/80 - loss: 0.5351 - acc: 0.9104 - 3s/step\n",
            "step 70/80 - loss: 0.0626 - acc: 0.9125 - 3s/step\n",
            "step 80/80 - loss: 0.0907 - acc: 0.9078 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.5226 - acc: 0.8125 - 1s/step\n",
            "step 20/20 - loss: 0.0254 - acc: 0.8250 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 14/100\n",
            "step 10/80 - loss: 0.5138 - acc: 0.8375 - 3s/step\n",
            "step 20/80 - loss: 0.2163 - acc: 0.8500 - 3s/step\n",
            "step 30/80 - loss: 0.1250 - acc: 0.8750 - 3s/step\n",
            "step 40/80 - loss: 0.1327 - acc: 0.8844 - 3s/step\n",
            "step 50/80 - loss: 0.1317 - acc: 0.8925 - 3s/step\n",
            "step 60/80 - loss: 0.0259 - acc: 0.8979 - 3s/step\n",
            "step 70/80 - loss: 0.0624 - acc: 0.9000 - 3s/step\n",
            "step 80/80 - loss: 0.4009 - acc: 0.9016 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.2267 - acc: 0.9250 - 1s/step\n",
            "step 20/20 - loss: 0.0066 - acc: 0.9250 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 15/100\n",
            "step 10/80 - loss: 0.0213 - acc: 0.9000 - 3s/step\n",
            "step 20/80 - loss: 0.0650 - acc: 0.9062 - 3s/step\n",
            "step 30/80 - loss: 0.1805 - acc: 0.9083 - 3s/step\n",
            "step 40/80 - loss: 0.1477 - acc: 0.9094 - 3s/step\n",
            "step 50/80 - loss: 0.0388 - acc: 0.9200 - 3s/step\n",
            "step 60/80 - loss: 0.0404 - acc: 0.9208 - 3s/step\n",
            "step 70/80 - loss: 0.4125 - acc: 0.9179 - 3s/step\n",
            "step 80/80 - loss: 0.0814 - acc: 0.9141 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.4246 - acc: 0.9000 - 1s/step\n",
            "step 20/20 - loss: 0.0041 - acc: 0.9062 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 16/100\n",
            "step 10/80 - loss: 0.0695 - acc: 0.9000 - 3s/step\n",
            "step 20/80 - loss: 0.2345 - acc: 0.8688 - 3s/step\n",
            "step 30/80 - loss: 0.1072 - acc: 0.8875 - 3s/step\n",
            "step 40/80 - loss: 0.0552 - acc: 0.9062 - 3s/step\n",
            "step 50/80 - loss: 0.1817 - acc: 0.9050 - 3s/step\n",
            "step 60/80 - loss: 0.0501 - acc: 0.9062 - 3s/step\n",
            "step 70/80 - loss: 0.3185 - acc: 0.9054 - 3s/step\n",
            "step 80/80 - loss: 0.1890 - acc: 0.9031 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.2026 - acc: 0.9000 - 1s/step\n",
            "step 20/20 - loss: 0.0286 - acc: 0.9187 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 17/100\n",
            "step 10/80 - loss: 0.5701 - acc: 0.9375 - 3s/step\n",
            "step 20/80 - loss: 0.0059 - acc: 0.9375 - 3s/step\n",
            "step 30/80 - loss: 0.0207 - acc: 0.9333 - 3s/step\n",
            "step 40/80 - loss: 0.1817 - acc: 0.9437 - 3s/step\n",
            "step 50/80 - loss: 0.3171 - acc: 0.9375 - 3s/step\n",
            "step 60/80 - loss: 0.3063 - acc: 0.9313 - 3s/step\n",
            "step 70/80 - loss: 0.3117 - acc: 0.9321 - 3s/step\n",
            "step 80/80 - loss: 0.0581 - acc: 0.9328 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.2165 - acc: 0.9375 - 1s/step\n",
            "step 20/20 - loss: 0.0068 - acc: 0.9375 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 18/100\n",
            "step 10/80 - loss: 0.4760 - acc: 0.8500 - 3s/step\n",
            "step 20/80 - loss: 0.2279 - acc: 0.8688 - 3s/step\n",
            "step 30/80 - loss: 0.0965 - acc: 0.8917 - 3s/step\n",
            "step 40/80 - loss: 0.3455 - acc: 0.9062 - 3s/step\n",
            "step 50/80 - loss: 0.6296 - acc: 0.9050 - 3s/step\n",
            "step 60/80 - loss: 0.0767 - acc: 0.9104 - 3s/step\n",
            "step 70/80 - loss: 1.1208 - acc: 0.9071 - 3s/step\n",
            "step 80/80 - loss: 0.0701 - acc: 0.9094 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.1822 - acc: 0.9125 - 1s/step\n",
            "step 20/20 - loss: 0.0237 - acc: 0.9125 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 19/100\n",
            "step 10/80 - loss: 0.1983 - acc: 0.8875 - 3s/step\n",
            "step 20/80 - loss: 0.0100 - acc: 0.9125 - 3s/step\n",
            "step 30/80 - loss: 0.0743 - acc: 0.9083 - 3s/step\n",
            "step 40/80 - loss: 0.1231 - acc: 0.8875 - 3s/step\n",
            "step 50/80 - loss: 0.0880 - acc: 0.8975 - 3s/step\n",
            "step 60/80 - loss: 0.1024 - acc: 0.9021 - 3s/step\n",
            "step 70/80 - loss: 0.2653 - acc: 0.8982 - 3s/step\n",
            "step 80/80 - loss: 0.2382 - acc: 0.9000 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.2207 - acc: 0.9250 - 1s/step\n",
            "step 20/20 - loss: 0.0623 - acc: 0.9250 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 20/100\n",
            "step 10/80 - loss: 0.1929 - acc: 0.9375 - 3s/step\n",
            "step 20/80 - loss: 0.3509 - acc: 0.9250 - 3s/step\n",
            "step 30/80 - loss: 0.3947 - acc: 0.9292 - 3s/step\n",
            "step 40/80 - loss: 0.1366 - acc: 0.9344 - 3s/step\n",
            "step 50/80 - loss: 0.0697 - acc: 0.9375 - 3s/step\n",
            "step 60/80 - loss: 0.1472 - acc: 0.9271 - 3s/step\n",
            "step 70/80 - loss: 0.2056 - acc: 0.9286 - 3s/step\n",
            "step 80/80 - loss: 0.0504 - acc: 0.9297 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.1763 - acc: 0.9250 - 1s/step\n",
            "step 20/20 - loss: 0.0162 - acc: 0.9250 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 21/100\n",
            "step 10/80 - loss: 0.3835 - acc: 0.9250 - 3s/step\n",
            "step 20/80 - loss: 0.2249 - acc: 0.9187 - 3s/step\n",
            "step 30/80 - loss: 0.0811 - acc: 0.9000 - 3s/step\n",
            "step 40/80 - loss: 0.6980 - acc: 0.8812 - 3s/step\n",
            "step 50/80 - loss: 0.0295 - acc: 0.8650 - 3s/step\n",
            "step 60/80 - loss: 0.4098 - acc: 0.8729 - 3s/step\n",
            "step 70/80 - loss: 0.1000 - acc: 0.8821 - 3s/step\n",
            "step 80/80 - loss: 0.2577 - acc: 0.8844 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.4442 - acc: 0.8375 - 1s/step\n",
            "step 20/20 - loss: 0.0313 - acc: 0.8500 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 22/100\n",
            "step 10/80 - loss: 0.3253 - acc: 0.8375 - 3s/step\n",
            "step 20/80 - loss: 0.2854 - acc: 0.8688 - 3s/step\n",
            "step 30/80 - loss: 0.3860 - acc: 0.8750 - 3s/step\n",
            "step 40/80 - loss: 0.0858 - acc: 0.8938 - 3s/step\n",
            "step 50/80 - loss: 0.1170 - acc: 0.9100 - 3s/step\n",
            "step 60/80 - loss: 0.0149 - acc: 0.9167 - 3s/step\n",
            "step 70/80 - loss: 0.0794 - acc: 0.9143 - 3s/step\n",
            "step 80/80 - loss: 1.0018 - acc: 0.9094 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.3691 - acc: 0.9000 - 1s/step\n",
            "step 20/20 - loss: 0.0149 - acc: 0.9125 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 23/100\n",
            "step 10/80 - loss: 0.2868 - acc: 0.8875 - 3s/step\n",
            "step 20/80 - loss: 0.6440 - acc: 0.9062 - 3s/step\n",
            "step 30/80 - loss: 0.1080 - acc: 0.9083 - 3s/step\n",
            "step 40/80 - loss: 0.0305 - acc: 0.9250 - 3s/step\n",
            "step 50/80 - loss: 0.0164 - acc: 0.9325 - 3s/step\n",
            "step 60/80 - loss: 0.0469 - acc: 0.9396 - 3s/step\n",
            "step 70/80 - loss: 0.0666 - acc: 0.9393 - 3s/step\n",
            "step 80/80 - loss: 0.2986 - acc: 0.9297 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.2615 - acc: 0.9000 - 1s/step\n",
            "step 20/20 - loss: 0.0709 - acc: 0.9187 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 24/100\n",
            "step 10/80 - loss: 0.1350 - acc: 0.9250 - 3s/step\n",
            "step 20/80 - loss: 0.5169 - acc: 0.9125 - 3s/step\n",
            "step 30/80 - loss: 0.1149 - acc: 0.9083 - 3s/step\n",
            "step 40/80 - loss: 0.0901 - acc: 0.9094 - 3s/step\n",
            "step 50/80 - loss: 0.0053 - acc: 0.9075 - 3s/step\n",
            "step 60/80 - loss: 0.2227 - acc: 0.9083 - 3s/step\n",
            "step 70/80 - loss: 0.0182 - acc: 0.9179 - 3s/step\n",
            "step 80/80 - loss: 0.0544 - acc: 0.9219 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.2692 - acc: 0.9375 - 1s/step\n",
            "step 20/20 - loss: 0.0063 - acc: 0.9313 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 25/100\n",
            "step 10/80 - loss: 0.1021 - acc: 0.9000 - 3s/step\n",
            "step 20/80 - loss: 0.0143 - acc: 0.9313 - 3s/step\n",
            "step 30/80 - loss: 0.0336 - acc: 0.9292 - 3s/step\n",
            "step 40/80 - loss: 0.0238 - acc: 0.9250 - 3s/step\n",
            "step 50/80 - loss: 0.0257 - acc: 0.9350 - 3s/step\n",
            "step 60/80 - loss: 0.3239 - acc: 0.9375 - 3s/step\n",
            "step 70/80 - loss: 0.0614 - acc: 0.9375 - 3s/step\n",
            "step 80/80 - loss: 0.3271 - acc: 0.9359 - 3s/step\n",
            "Eval begin...\n",
            "step 10/20 - loss: 0.2636 - acc: 0.9000 - 1s/step\n",
            "step 20/20 - loss: 0.0602 - acc: 0.9187 - 1s/step\n",
            "Eval samples: 160\n",
            "Epoch 26/100\n",
            "step 10/80 - loss: 0.1041 - acc: 0.9750 - 3s/step\n",
            "step 20/80 - loss: 0.4471 - acc: 0.9375 - 3s/step\n"
          ]
        }
      ],
      "source": [
        "model.fit(\n",
        "    Fit_dataloader,\n",
        "    Eval_dataloader,\n",
        "    epochs=Train_Paramdict['epoches']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V19HnpEheuLT"
      },
      "source": [
        "## 预测"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ue9m2Rve5AA"
      },
      "outputs": [],
      "source": [
        "# 数据加载\n",
        "Test_dataset = Test_Dataset(Test_data)\n",
        "Test_dataloader = DataLoader(Test_dataset, batch_size=Train_Paramdict['batchsize'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzPEkjK6IXyx"
      },
      "outputs": [],
      "source": [
        "results = model.predict(Test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYvQv6qGIYOo"
      },
      "outputs": [],
      "source": [
        "results = np.asarray(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agUbO4zBIaNM"
      },
      "outputs": [],
      "source": [
        "import paddle.nn.functional as F\n",
        "\n",
        "submit_result = []\n",
        "for i in results[0]:\n",
        "    i = paddle.to_tensor(i)\n",
        "    i = F.softmax(i)\n",
        "    result = i[:, 1]\n",
        "    submit_result += result.numpy().tolist()\n",
        "len(submit_result)\n",
        "\n",
        "submit_result = np.asarray(submit_result)\n",
        "\n",
        "Test_data.iloc[:, 1] = submit_result\n",
        "Test_data.head()\n",
        "\n",
        "Submit_data = Test_data.copy()\n",
        "Submit_data.head()\n",
        "\n",
        "Submit_data.columns = ['FileName', 'PM Risk']\n",
        "Submit_data.head()\n",
        "\n",
        "for i in range(len(Submit_data)):\n",
        "    Submit_data.iloc[i, 0] = Submit_data.iloc[i, 0][-9:]\n",
        "Submit_data.head()\n",
        "\n",
        "Submit_data.to_csv('PALM_PaddleX_10/Classification_Results.csv', index=False, float_format=\"%.1f\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PALM_PaddleX_10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMEkp3+hopxz9E93zXIB4cg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}